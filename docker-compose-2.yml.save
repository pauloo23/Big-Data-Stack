x-superset-image: &superset-image apache/superset:${TAG:-latest-dev}
x-superset-user: &superset-user root
x-superset-depends-on: &superset-depends-on
  - db
  - redis
x-superset-volumes: &superset-volumes
  # /app/pythonpath_docker will be appended to the PYTHONPATH in the final container
  - ./superset/docker:/app/docker
  - ./superset/superset:/app/superset
  - ./superset/superset-frontend:/app/superset-frontend
  - superset_home:/app/superset_home
  - ./superset/tests:/app/tests
  
  
version: '3.7'

services:
  # Hadoop master
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
    volumes:
      - ./namenode/home/${ADMIN_NAME:?err}:/home/${ADMIN_NAME:?err}
      - ./namenode/hadoop-data:/hadoop-data
      - ./namenode/entrypoint.sh:/entrypoint.sh
      - hadoop-namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop

  # Hadoop slave 1
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    volumes:
      - hadoop-datanode-1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode

  # Hadoop slave 2
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    volumes:
      - hadoop-datanode-2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode

  resourcemanager:
    restart: always
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    ports:
      - 8088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      - datanode2

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    volumes:
      - ./nodemanagers/entrypoint.sh:/entrypoint.sh
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      

  nodemanager2:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager2
    volumes:
      - ./nodemanagers/entrypoint.sh:/entrypoint.sh
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode2:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode2

  # add datanode2:9864 to service precondition
  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    ports:
      - 8188:8188
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    volumes:
      - hadoop-historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      - datanode2

  # HUE (Management node, similar to Ambari)
  hue:
    container_name: hue
    image: gethue/hue:4.4.0
    ports:
      - 8000:8888
    env_file:
      - ./hadoop.env
    volumes:
      - ./hue/hue-overrides.ini:/usr/share/hue/desktop/conf/hue-overrides.ini
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop

  hue-db:
    container_name: hue-db
    restart: always
    image: postgres:11.9
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=secret
      - POSTGRES_DB=hue
    ports:
      - 5432:5432
    volumes:
      - hue-db:/var/lib/postgresql/data
      - ./hue-db/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - hadoop

  # HIVE
hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"

  #add datanode2:9864 to service precondition
  hive-metastore:
    container_name: hivemetastore
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ./hadoop.env
    command: /opt/hive/bin/hive --service metastore
    networks:
      - hadoop
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 hive-metastore-postgresql:5432"

  hive-metastore-postgresql:
    container_name: hivedb
    image: bde2020/hive-metastore-postgresql:2.3.0
    volumes:
      - hive-metastore:/var/lib/postgresql/data
    networks:
      - hadoop

  presto-coordinator:
    container_name: presto_coordinator
    image: shawnzhu/prestodb:0.181
    networks:
      - hadoop

  # Spark
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  spark-worker-1:
    image:  bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  spark-worker-2:
    image:  bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - 8082:8081
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
  # Spark notebooks
  jupyter-spark:
    # To see all running servers in this container, execute
    # `docker exec jupyter-spark jupyter notebook list`
    container_name: jupyter-spark
    build:
      context: jupyter-spark
      args:
        - SPARK_VERSION=3.1.1
        - HADOOP_VERSION=3.2
        - SPARK_CHECKSUM=E90B31E58F6D95A42900BA4D288261D71F6C19FA39C1CB71862B792D1B5564941A320227F6AB0E09D946F16B8C1969ED2DEA2A369EC8F9D2D7099189234DE1BE
        - OPENJDK_VERSION=11
        # Make sure the python version in the driver (the notebooks) is the same as in spark-master,
        # spark-worker-1, and spark-worker-2
        - PYTHON_VERSION=3.7.10
    ports:
      - 8888:8888
      - 8889:8889
      - 4040:4040
      - 4041:4041
    volumes:
      - ./jupyter-spark/work:/home/jovyan/work
      - ./jupyter-spark/drivers:/drivers
    pid: host
    environment:
      - TINI_SUBREAPER=true
      - GRANT_SUDO=yes
    env_file:
      - ./hadoop.env
    networks:
      - hadoop


  # Superset   
  redis:
    image: redis:latest
    container_name: superset_cache
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis:/data

  db:
    env_file: superset/docker/.env
    image: postgres:10
    container_name: superset_db
    restart: unless-stopped
    ports:
      - "127.0.0.1:5432:5432"
    volumes:
      - db_home:/var/lib/postgresql/data

  superset:
    env_file: superset/docker/.env
    image: *superset-image
    container_name: superset_app
    command: ["/superset/app/docker/docker-bootstrap.sh", "app"]
    restart: unless-stopped
    ports:
      - 8088:8088
    user: *superset-user
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    environment:
      CYPRESS_CONFIG: "${CYPRESS_CONFIG}"

  superset-websocket:
    container_name: superset_websocket
    build: ./superset/superset-websocket
    image: superset-websocket
    ports:
      - 8080:8080
    depends_on:
      - redis
    # Mount everything in superset-websocket into container and
    # then exclude node_modules and dist with bogus volume mount.
    # This is necessary because host and container need to have
    # their own, separate versions of these files. .dockerignore
    # does not seem to work when starting the service through
    # docker-compose.
    #
    # For example, node_modules may contain libs with native bindings.
    # Those bindings need to be compiled for each OS and the container
    # OS is not necessarily the same as host OS.
    volumes:
      - ./superset-websocket:/home/superset-websocket
      - /home/superset-websocket/node_modules
      - /home/superset-websocket/dist
    environment:
      - PORT=8080
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_SSL=false

  superset-init:
    image: *superset-image
    container_name: superset_init
    command: ["/superset/app/docker/docker-init.sh"]
    env_file: superset/docker/.env
    depends_on: *superset-depends-on
    user: *superset-user
    volumes: *superset-volumes
    environment:
      CYPRESS_CONFIG: "${CYPRESS_CONFIG}"

  superset-node:
    image: node:16
    container_name: superset_node
    command: ["/superset/app/docker/docker-frontend.sh"]
    env_file: superset/docker/.env
    depends_on: *superset-depends-on
    volumes: *superset-volumes

  superset-worker:
    image: *superset-image
    container_name: superset_worker
    command: ["/app/docker/docker-bootstrap.sh", "worker"]
    env_file: superset/docker/.env
    restart: unless-stopped
    depends_on: *superset-depends-on
    user: *superset-user
    volumes: *superset-volumes
    # Bump memory limit if processing selenium / thumbails on superset-worker
    # mem_limit: 2038m
    # mem_reservation: 128M

  superset-worker-beat:
    image: *superset-image
    container_name: superset_worker_beat
    command: ["/superset/app/docker/docker-bootstrap.sh", "beat"]
    env_file: superset/docker/.env
    restart: unless-stopped
    depends_on: *superset-depends-on
    user: *superset-user
    volumes: *superset-volumes

  superset-tests-worker:
    image: *superset-image
    container_name: superset_tests_worker
    command: ["/superset/app/docker/docker-bootstrap.sh", "worker"]
    env_file: superset/docker/.env
    environment:
      DATABASE_HOST: localhost
      DATABASE_DB: test
      REDIS_CELERY_DB: 2
      REDIS_RESULTS_DB: 3
      REDIS_HOST: localhost
    network_mode: host
    depends_on: *superset-depends-on
    user: *superset-user
    volumes: *superset-volumes

  timescaledb:
    build: "docker/postgres"
    ports:
      - "7432:5432"
    environment:
      POSTGRES_MULTIPLE_DATABASES: "mlflow"
      POSTGRES_USER: "postgres"
      POSTGRES_PASSWORD: "postgres"
      MAX_CONNECTIONS: 100
      SHARED_BUFFERS: "1GB"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - hadoop
 
  portainer:
    image: portainer/portainer-ce:2.9.2
    restart: unless-stopped
    command: -H unix:///var/run/docker.sock
    ports:
      - 443:9443
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - dataportainer:/data
    environment:
      TZ: "Europe/Lisbon"


  datascience-notebook:
    image: jupyter/tensorflow-notebook
    container_name: datascience-notebook
    environment:
      PASSWORD: ${PASSWORD:-ChangeMe!}
    restart: always
    ports:
      - 28888:8888
    volumes:
      - ./jupyter-tensorflow/work:/home/jovyan/work
    environment:
      - TINI_SUBREAPER=true
      - GRANT_SUDO=yes
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

# RUN apt-get update
# RUN apt-get install default-jdk -y
# docker exec -it -u root container_id bash
 

  prestodb:
    build: "prestodb"
    ports:
      - "8083:8080"
      - "8182:8181"    
    networks:
      - hadoop

networks:
  hadoop:

volumes:
  hadoop-namenode:
  hadoop-datanode-1:
  hadoop-datanode-2:
 # hadoop-datanode-3:
  hadoop-historyserver:
  hue-db:
  hive-metastore:
  superset_home:
    external: false
  db_home:
    external: false
  redis:
    external: false
  postgres-db-volume:
  dataportainer:
