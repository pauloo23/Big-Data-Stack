 
version: '3.7'

services:
  # Hadoop master
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
    volumes:
      - ./namenode/home/${ADMIN_NAME:?err}:/home/${ADMIN_NAME:?err}
      - ./namenode/hadoop-data:/hadoop-data
      - ./namenode/entrypoint.sh:/entrypoint.sh
      - hadoop-namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop

  # Hadoop slave 1
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    volumes:
      - hadoop-datanode-1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
    ports:
      - "50075:50075"

  # Hadoop slave 2
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    volumes:
      - hadoop-datanode-2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
    ports:
      - "50074:50074"

  resourcemanager:
    restart: always
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    ports:
      - 8088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      - datanode2

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    volumes:
      - ./nodemanagers/entrypoint.sh:/entrypoint.sh
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      

  nodemanager2:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager2
    volumes:
      - ./nodemanagers/entrypoint.sh:/entrypoint.sh
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode2:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
      - .env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode2

  # add datanode2:9864 to service precondition
  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    ports:
      - 8188:8188
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    volumes:
      - hadoop-historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode1
      - datanode2

  # HUE (Management node, similar to Ambari)
  hue:
    container_name: hue
    image: gethue/hue:4.4.0
    ports:
      - 8000:8888
    env_file:
      - ./hadoop.env
    volumes:
      - ./hue/hue-overrides.ini:/usr/share/hue/desktop/conf/hue-overrides.ini
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop

  hue-db:
    container_name: hue-db
    restart: always
    image: postgres:11.9
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=secret
      - POSTGRES_DB=hue
    ports:
      - 5432:5432
    volumes:
      - hue-db:/var/lib/postgresql/data
      - ./hue-db/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - hadoop

  # HIVE
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    networks:
      - hadoop


  #add datanode2:9864 to service precondition
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    networks:
      - hadoop

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0 
    ports:
      - "5433:5432"
    networks:
      - hadoop

  presto-coordinator:
    container_name: presto_coordinator
    image: shawnzhu/prestodb:0.181
    networks:
      - hadoop

  # Spark
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  spark-worker-1:
    image:  bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  spark-worker-2:
    image:  bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - 8082:8081
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    env_file:
      - ./hadoop.env
    networks:
      - hadoop


  portainer:
    image: portainer/portainer-ce:2.9.2
    restart: unless-stopped
    command: -H unix:///var/run/docker.sock
    ports:
      - 443:9443
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - dataportainer:/data
    environment:
      TZ: "Europe/Lisbon"


  datascience-notebook:
    image: jupyter/tensorflow-notebook
    container_name: datascience-notebook
    environment:
      PASSWORD: ${PASSWORD:-ChangeMe!}
    restart: always
    ports:
      - 28888:8888
    volumes:
      - ./jupyter-tensorflow/work:/home/jovyan/work
    environment:
      - TINI_SUBREAPER=true
      - GRANT_SUDO=yes
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

# RUN apt-get update
# RUN apt-get install default-jdk -y
# docker exec -it -u root container_id bash
 

  prestodb:
    build: "prestodb"
    ports:
      - "8083:8080"
      - "8182:8181"    
    networks:
      - hadoop

##nifi dependencies
    zookeeper:
        container_name: zookeeper_container_persistent
        image: 'bitnami/zookeeper:3.7.0'  # latest image as of 2021-11-09.
        restart: on-failure
        environment:
            - ALLOW_ANONYMOUS_LOGIN=yes
        networks:
            - hadoop
# version control for nifi flows
    registry:
        hostname: myregistry
        container_name: registry_container_persistent
        image: 'apache/nifi-registry:1.15.0'  # latest image as of 2021-11-09.
        restart: on-failure
        ports:
            - "18080:18080"
        environment:
            - LOG_LEVEL=INFO
            - NIFI_REGISTRY_DB_DIR=/opt/nifi-registry/nifi-registry-current/database
            - NIFI_REGISTRY_FLOW_PROVIDER=file
            - NIFI_REGISTRY_FLOW_STORAGE_DIR=/opt/nifi-registry/nifi-registry-current/flow_storage
        volumes:
            - ./nifi_registry/database:/opt/nifi-registry/nifi-registry-current/database
            - ./nifi_registry/flow_storage:/opt/nifi-registry/nifi-registry-current/flow_storage
        networks:
            - hadoop
            
# data extraction, transformation and load service
    nifi:
        container_name: nifi_container_persistent
        image: 'apache/nifi:1.14.0'  # latest image as of 2021-11-09.
        restart: on-failure
        ports:
            - '8091:9090'
        environment:
            - NIFI_WEB_HTTP_PORT=9090
            - NIFI_CLUSTER_IS_NODE=true
            - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
            - NIFI_ZK_CONNECT_STRING=myzookeeper:2181
            - NIFI_ELECTION_MAX_WAIT=30 sec
            - NIFI_SENSITIVE_PROPS_KEY='12345678901234567890A'
        healthcheck:
            test: "${DOCKER_HEALTHCHECK_TEST:-curl localhost:8091/nifi/}"
            interval: "60s"
            timeout: "3s"
            start_period: "5s"
            retries: 5
        volumes:
            - ./nifi/database_repository:/opt/nifi/nifi-current/database_repository
            - ./nifi/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
            - ./nifi/content_repository:/opt/nifi/nifi-current/content_repository
            - ./nifi/provenance_repository:/opt/nifi/nifi-current/provenance_repository
            - ./nifi/state:/opt/nifi/nifi-current/state
            - ./nifi/logs:/opt/nifi/nifi-current/logs
            # uncomment the next line after copying the /conf directory from the container to your local directory to persist NiFi flows
            #- ./nifi/conf:/opt/nifi/nifi-current/conf
        networks:
            - hadoop
            
    superset: 
        container_name: superset
        image: 'tylerfowler/superset'  # latest image of typerflow/superset
        restart: always
        ports:
            - '8098:8098'
        environment:
            - ADMIN_USERNAME=${ADMIN_USERNAME}
            - ADMIN_FIRST_NAME=${ADMIN_FIRST_NAME}
            - ADMIN_LAST_NAME=${ADMIN_LAST_NAME}
            - ADMIN_EMAIL=${ADMIN_EMAIL}
            - ADMIN_PWD=${ADMIN_PWD}
        networks:
            - hadoop

networks:
  hadoop:

volumes:
  hadoop-namenode:
  hadoop-datanode-1:
  hadoop-datanode-2:
  hadoop-historyserver:
  hue-db:
  postgres-db-volume:
  dataportainer:
